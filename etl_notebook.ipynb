{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/news_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "import functions_framework \n",
    "\n",
    "try:\n",
    "    # If main.py is in the root directory\n",
    "    from components.extract_content import extract_data\n",
    "    from components.transform_content import transform_data\n",
    "    from components.analyse_sentiment import analyze_sentiment\n",
    "    from components.load_content import load_data_to_bigquery\n",
    "    from components.logging_config import setup_logging, get_logger\n",
    "except ImportError:\n",
    "    # If main.py is somewhere else\n",
    "    import sys\n",
    "    import os\n",
    "    # Get the absolute path to the project root directory\n",
    "    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))\n",
    "    sys.path.append(project_root)\n",
    "    \n",
    "    # Try again with the updated path\n",
    "    from components.extract_content import extract_data\n",
    "    from components.transform_content import transform_data\n",
    "    from components.analyse_sentiment import analyze_sentiment\n",
    "    from components.load_content import load_data_to_bigquery\n",
    "    from components.logging_config import setup_logging, get_logger \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "setup_logging()\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "# Configure logging for cloud environment (avoid file handlers in Cloud Functions)\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[logging.StreamHandler()]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 21:51:08,895 - __main__ - INFO - News ETL pipeline triggered at 2025-04-27T21:51:08.895024\n"
     ]
    }
   ],
   "source": [
    "# Log function start\n",
    "logger.info(f\"News ETL pipeline triggered at {datetime.now().isoformat()}\")\n",
    "\n",
    " # Get configuration from environment variables\n",
    "service_account_path = os.environ.get(\"GOOGLE_APPLICATION_CREDENTIALS\")\n",
    "load_method = os.environ.get(\"LOAD_METHOD\", \"append\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Extract data from NewsAPI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 21:51:34,470 - __main__ - INFO - Step 1: Extracting news data...\n",
      "2025-04-27 21:51:34,473 - components.extract_content - INFO - Current date: 2025-04-27\n",
      "2025-04-27 21:51:34,475 - components.extract_content - INFO - 7 days ago: 2025-04-20\n",
      "2025-04-27 21:51:34,479 - components.extract_content - INFO - Fetching articles for topic: GenAI...\n",
      "2025-04-27 21:51:37,015 - components.extract_content - INFO - Fetched 100 articles for topic: GenAI\n",
      "2025-04-27 21:51:37,016 - components.extract_content - INFO - Fetching articles for topic: AI...\n",
      "2025-04-27 21:51:37,444 - components.extract_content - INFO - Fetched 99 articles for topic: AI\n",
      "2025-04-27 21:51:37,444 - components.extract_content - INFO - Fetching articles for topic: Technology...\n",
      "2025-04-27 21:51:38,281 - components.extract_content - INFO - Fetched 100 articles for topic: Technology\n",
      "2025-04-27 21:51:38,282 - components.extract_content - INFO - Total unique articles fetched: 289\n",
      "2025-04-27 21:51:38,283 - __main__ - INFO - Extracted 289 articles\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Step 1: Extracting news data...\")\n",
    "articles = extract_data()\n",
    "logger.info(f\"Extracted {len(articles)} articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Transform the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 21:53:25,435 - __main__ - INFO - Step 2: Transforming news data...\n",
      "2025-04-27 21:53:25,442 - components.transform_content - INFO - Transforming data...\n",
      "2025-04-27 21:53:25,476 - components.transform_content - INFO - After removing duplicates: 275 articles\n",
      "/Users/dhruvpatel/Desktop/projects/news_Insights_dashboard/components/transform_content.py:146: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  final_df['publishedAt'] = pd.to_datetime(final_df['publishedAt']).dt.strftime('%Y-%m-%d')\n",
      "2025-04-27 21:53:25,500 - components.transform_content - INFO - Extracting full content from URLs (this may take a while)...\n",
      "100%|██████████| 275/275 [02:34<00:00,  1.78it/s]\n",
      "/Users/dhruvpatel/Desktop/projects/news_Insights_dashboard/components/transform_content.py:156: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  final_df['full_content'] = [extract_content(url) for url in tqdm(final_df['url'])]\n",
      "2025-04-27 21:56:00,514 - __main__ - INFO - Transformed data: 275 articles\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Step 2: Transforming news data...\")\n",
    "transformed_df = transform_data(articles)\n",
    "logger.info(f\"Transformed data: {len(transformed_df)} articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Step 3: Analyze sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 21:56:00,526 - __main__ - INFO - Step 3: Analyzing sentiment...\n",
      "2025-04-27 21:56:00,526 - components.analyse_sentiment - INFO - Initializing sentiment analysis pipeline...\n",
      "Device set to use mps:0\n",
      "2025-04-27 21:56:01,463 - components.analyse_sentiment - INFO - Sentiment analysis pipeline initialized successfully\n",
      "2025-04-27 21:56:01,465 - components.analyse_sentiment - INFO - Applying sentiment analysis to 275 articles...\n",
      "100%|██████████| 9/9 [00:11<00:00,  1.24s/it]\n",
      "2025-04-27 21:56:12,641 - components.analyse_sentiment - INFO - Sentiment analysis completed successfully\n",
      "2025-04-27 21:56:12,647 - __main__ - INFO - Sentiment analysis complete\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Step 3: Analyzing sentiment...\")\n",
    "final_df = analyze_sentiment(transformed_df)\n",
    "logger.info(\"Sentiment analysis complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Load data to BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 22:12:19,412 - __main__ - INFO - Step 4: Loading data to BigQuery...\n",
      "2025-04-27 22:12:19,430 - components.load_content - INFO - BigQuery Configuration - Project: upheld-quanta-455417-m4, Dataset: news_dataset, Table: news_articles\n",
      "2025-04-27 22:12:19,431 - components.load_content - INFO - Service account file not found or not specified. Using default credentials.\n",
      "/Users/dhruvpatel/Desktop/projects/news_Insights_dashboard/components/load_content.py:73: FutureWarning: to_gbq is deprecated and will be removed in a future version. Please use pandas_gbq.to_gbq instead: https://pandas-gbq.readthedocs.io/en/latest/api.html#pandas_gbq.to_gbq\n",
      "  dataframe.to_gbq(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please visit this URL to authorize this application: https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=725825577420-unm2gnkiprugilg743tkbig250f4sfsj.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8080%2F&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fbigquery&state=k8EZrVoFF8LBSPl3CPMjvqKmrOOLdH&prompt=consent&access_type=offline\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 22:12:31,829 - google_auth_oauthlib.flow - INFO - \"GET /?state=k8EZrVoFF8LBSPl3CPMjvqKmrOOLdH&code=4/0Ab_5qlk-3m0G2KFknSj4jF76rTHLvr3JYkLgwNQ2hMwE65Zu7EwtTcFx9NNAutTiRLuNYg&scope=https://www.googleapis.com/auth/bigquery HTTP/1.1\" 200 65\n",
      "275 out of 275 rows loaded.<?, ?it/s]2025-04-27 22:12:36,267 - pandas_gbq.gbq - INFO - \n",
      "100%|██████████| 1/1 [00:00<00:00, 1216.45it/s]\n",
      "2025-04-27 22:12:36,269 - components.load_content - INFO - Successfully loaded 275 rows to news_dataset.news_articles\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Step 4: Loading data to BigQuery...\")\n",
    "rows_loaded = load_data_to_bigquery(\n",
    "            final_df,\n",
    "            service_account_path=service_account_path,\n",
    "            method=load_method\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 22:18:43,729 - __main__ - INFO - Successfully loaded 275 rows to BigQuery\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"Successfully loaded {rows_loaded} rows to BigQuery\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "news_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
