{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extract news from the Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/news_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import datetime\n",
    "import os\n",
    "import ast\n",
    "import logging\n",
    "from typing import List, Dict, Any, Optional, Union\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from newsapi import NewsApiClient\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from transformers import pipeline \n",
    "\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract_data function : \n",
    "\n",
    "def extract_data():\n",
    "    \"\"\"\n",
    "    Extract news articles from NewsAPI for multiple topics and combine them.\n",
    "    \n",
    "    Returns:\n",
    "        list: Combined list of articles from different topics\n",
    "    \"\"\"\n",
    "    import datetime \n",
    "    import os \n",
    "    from dotenv import load_dotenv\n",
    "    from newsapi import NewsApiClient \n",
    "    \n",
    "    # Get current date and date from 7 days ago\n",
    "    current_date = datetime.datetime.now().strftime('%Y-%m-%d')\n",
    "    print(\"Current date:\", current_date)\n",
    "    \n",
    "    seven_days_ago = (datetime.datetime.now() - datetime.timedelta(days=7)).strftime('%Y-%m-%d')\n",
    "    print(\"7 days ago:\", seven_days_ago)\n",
    "    \n",
    "    # Load API key from .env file\n",
    "    load_dotenv()\n",
    "    news_api = os.getenv(\"NEWS_API\")\n",
    "    \n",
    "    # Initialize NewsAPI client\n",
    "    newsapi = NewsApiClient(news_api)\n",
    "    \n",
    "    # Define topics to search for\n",
    "    topics = ['GenAI', 'AI', 'Technology']\n",
    "    combined_articles = []\n",
    "    \n",
    "    # Fetch articles for each topic\n",
    "    for topic in topics:\n",
    "        articles = newsapi.get_everything(\n",
    "            q=topic,\n",
    "            from_param=seven_days_ago,\n",
    "            to=current_date,\n",
    "            language='en',\n",
    "            sort_by='relevancy',\n",
    "            page=2\n",
    "        )\n",
    "        \n",
    "        print(f\"Fetched {len(articles['articles'])} articles for topic: {topic}\")\n",
    "        combined_articles.extend(articles['articles'])\n",
    "    \n",
    "    # Remove duplicate articles (same URL)\n",
    "    seen_urls = set()\n",
    "    unique_articles = []\n",
    "    \n",
    "    for article in combined_articles:\n",
    "        if article['url'] not in seen_urls:\n",
    "            seen_urls.add(article['url'])\n",
    "            unique_articles.append(article)\n",
    "    \n",
    "    print(f\"Total unique articles fetched: {len(unique_articles)}\")\n",
    "    return unique_articles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform the data extracted from source "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_source_name(source):\n",
    "    \"\"\"\n",
    "    Extract the name from a source object which can be either a dictionary or string.\n",
    "    \n",
    "    Args:\n",
    "        source: Source object from NewsAPI (can be dict or string)\n",
    "        \n",
    "    Returns:\n",
    "        str: Extracted source name or 'Unknown' if not found\n",
    "    \"\"\"\n",
    "    if isinstance(source, dict):\n",
    "        return source.get('name', 'Unknown')\n",
    "    elif isinstance(source, str):\n",
    "        try:\n",
    "            dict_data = ast.literal_eval(source)  # Attempt to parse it as a dictionary\n",
    "            if isinstance(dict_data, dict):\n",
    "                return dict_data.get('name', 'Unknown')\n",
    "        except (SyntaxError, ValueError):\n",
    "            # If source is a plain string, return it directly\n",
    "            return source\n",
    "    return 'Unknown'\n",
    "\n",
    "def extract_content(url):\n",
    "    \"\"\"\n",
    "    Extract the main content from a webpage given its URL.\n",
    "    \n",
    "    Args:\n",
    "        url (str): URL of the article\n",
    "        \n",
    "    Returns:\n",
    "        str: Extracted text content or error message\n",
    "    \"\"\"\n",
    "    try:\n",
    "        headers = {\"User-Agent\": \"Mozilla/5.0\"}  # Mimic a browser request\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()  # Raise an error for bad responses\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        \n",
    "        # Extract meaningful text (modify based on website structure)\n",
    "        paragraphs = soup.find_all(\"p\")\n",
    "        content = \" \".join(p.text for p in paragraphs)\n",
    "        \n",
    "        return content[:1000]  # Return first 1000 characters to avoid large data\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "def transform_data(combined_articles):\n",
    "    \"\"\"\n",
    "    Transform the combined articles into a cleaned and structured DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        combined_articles (list): List of article dictionaries from NewsAPI\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: Transformed and cleaned DataFrame\n",
    "    \"\"\"\n",
    "    # Convert to DataFrame\n",
    "    combined_articles_df = pd.DataFrame(combined_articles)\n",
    "    \n",
    "    # Drop the urlToImage column\n",
    "    combined_articles_df = combined_articles_df.drop('urlToImage', axis=1)\n",
    "    \n",
    "    # Remove duplicates based on description\n",
    "    final_df = combined_articles_df.drop_duplicates(subset=[\"description\"], keep='first')\n",
    "    \n",
    "    # Format dates\n",
    "    final_df['publishedAt'] = pd.to_datetime(final_df['publishedAt']).dt.strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Apply source name extraction\n",
    "    final_df.loc[:, 'source'] = final_df['source'].apply(extract_source_name)\n",
    "    \n",
    "    # Extract full content from each article URL\n",
    "    final_df['full_content'] = final_df['url'].apply(extract_content) \n",
    "    final_df['publishedAt'] = pd.to_datetime(final_df['publishedAt'])\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def analyze_sentiment(df: pd.DataFrame, text_column: str = 'full_content') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply sentiment analysis to the specified text column in the DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing the text to analyze\n",
    "        text_column: Column name containing the text to analyze\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with sentiment analysis results added\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If the text column doesn't exist\n",
    "    \"\"\"\n",
    "    print(\"Initializing sentiment analysis pipeline...\")\n",
    "    \n",
    "    if text_column not in df.columns:\n",
    "        raise ValueError(f\"Text column '{text_column}' not found in DataFrame\")\n",
    "    \n",
    "    # Initialize sentiment analysis pipeline\n",
    "    try:\n",
    "        sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "        print(\"Sentiment analysis pipeline initialized successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing sentiment analysis pipeline: {str(e)}\")\n",
    "        raise\n",
    "    # Apply sentiment analysis in batches to avoid memory issues\n",
    "    print(f\"Applying sentiment analysis to {len(df)} articles...\")\n",
    "    batch_size = 32\n",
    "    results = []\n",
    "    \n",
    "    # Process in batches with progress bar\n",
    "    for i in tqdm(range(0, len(df), batch_size)):\n",
    "        batch = df[text_column].iloc[i:i+batch_size].fillna(\"\").tolist()\n",
    "        # Filter out empty strings\n",
    "        valid_texts = []\n",
    "        valid_indices = []\n",
    "        \n",
    "        for j, text in enumerate(batch):\n",
    "            if isinstance(text, str) and text.strip():\n",
    "                valid_texts.append(text)\n",
    "                valid_indices.append(j)\n",
    "        \n",
    "        if valid_texts:\n",
    "            try:\n",
    "                batch_results = sentiment_pipeline(valid_texts)\n",
    "                \n",
    "                # Place results in the correct positions\n",
    "                for idx, result in zip(valid_indices, batch_results):\n",
    "                    while len(results) < i + idx:\n",
    "                        results.append({\"label\": \"NEUTRAL\", \"score\": 0.5})\n",
    "                    results.append(result)\n",
    "            except Exception as e:\n",
    "                print(f\"Error in batch sentiment analysis: {str(e)}\")\n",
    "                # Add neutral results for this batch\n",
    "                results.extend([{\"label\": \"NEUTRAL\", \"score\": 0.5} for _ in valid_texts])\n",
    "        \n",
    "        # Fill in missing results for empty strings\n",
    "        while len(results) < min(i + batch_size, len(df)):\n",
    "            results.append({\"label\": \"NEUTRAL\", \"score\": 0.5})\n",
    "    \n",
    "    # Ensure we have exactly the right number of results\n",
    "    if len(results) != len(df):\n",
    "        print(f\"Results length mismatch: {len(results)} vs {len(df)}. Adjusting...\")\n",
    "        if len(results) < len(df):\n",
    "            results.extend([{\"label\": \"NEUTRAL\", \"score\": 0.5} for _ in range(len(df) - len(results))])\n",
    "        else:\n",
    "            results = results[:len(df)]\n",
    "    \n",
    "    # Add sentiment results to the dataframe\n",
    "    df.loc[:, 'sentiment_label'] = [result['label'] for result in results]\n",
    "    df.loc[:, 'sentiment_score'] = [result['score'] for result in results]\n",
    "    df.loc[:, 'sentiment_value'] = df['sentiment_label'].map({'POSITIVE': 1, 'NEGATIVE': -1, 'NEUTRAL': 0})\n",
    "    \n",
    "    print(\"Sentiment analysis completed successfully\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load to the big query function :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_to_bigquery(dataframe, service_account_path='./service_account.json', method='replace'):\n",
    "    \"\"\"\n",
    "    Load a pandas DataFrame to BigQuery.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dataframe : pandas.DataFrame\n",
    "        The DataFrame to be loaded to BigQuery\n",
    "    service_account_path : str, default='./service_account.json'\n",
    "        Path to the Google Cloud service account JSON file\n",
    "    method : str, default='replace'\n",
    "        What to do if the table exists. Options: 'fail', 'replace', or 'append'\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    int\n",
    "        Number of rows loaded to BigQuery\n",
    "    \"\"\"\n",
    "    # Load environment variables\n",
    "    load_dotenv()\n",
    "    \n",
    "    # Get BigQuery project, dataset, and table details from environment variables\n",
    "    project_id = os.getenv(\"project_id\")\n",
    "    dataset_id = os.getenv(\"dataset_id\")\n",
    "    table_id = os.getenv(\"table_id\")\n",
    "    \n",
    "    if not all([project_id, dataset_id, table_id]):\n",
    "        raise ValueError(\"Missing environment variables. Make sure project_id, dataset_id, and table_id are set.\")\n",
    "    \n",
    "    # Full table reference\n",
    "    table_ref = f\"{dataset_id}.{table_id}\"\n",
    "    \n",
    "    # Set up credentials\n",
    "    credentials = service_account.Credentials.from_service_account_file(service_account_path)\n",
    "    \n",
    "    # Check if publishedAt is in datetime format\n",
    "    if 'publishedAt' in dataframe.columns and dataframe['publishedAt'].dtype == 'object':\n",
    "        dataframe['publishedAt'] = pd.to_datetime(dataframe['publishedAt'])\n",
    "    \n",
    "    # Upload to BigQuery with error handling\n",
    "    try:\n",
    "        dataframe.to_gbq(\n",
    "            destination_table=table_ref,\n",
    "            project_id=project_id,\n",
    "            if_exists=method,\n",
    "            credentials=credentials\n",
    "        )\n",
    "        print(f\"Successfully loaded {len(dataframe)} rows to {table_ref}\")\n",
    "        return len(dataframe)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data to BigQuery: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# function call : \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current date: 2025-04-14\n",
      "7 days ago: 2025-04-07\n",
      "Fetched 100 articles for topic: GenAI\n",
      "Fetched 98 articles for topic: AI\n",
      "Fetched 100 articles for topic: Technology\n",
      "Total unique articles fetched: 290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_k/y3vww_pn5p14wn86g8x4k5mh0000gn/T/ipykernel_91847/3011008576.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  final_df['publishedAt'] = pd.to_datetime(final_df['publishedAt']).dt.strftime('%Y-%m-%d')\n",
      "/var/folders/_k/y3vww_pn5p14wn86g8x4k5mh0000gn/T/ipykernel_91847/3011008576.py:79: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  final_df['full_content'] = final_df['url'].apply(extract_content)\n",
      "/var/folders/_k/y3vww_pn5p14wn86g8x4k5mh0000gn/T/ipykernel_91847/3011008576.py:80: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  final_df['publishedAt'] = pd.to_datetime(final_df['publishedAt'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing sentiment analysis pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment analysis pipeline initialized successfully\n",
      "Applying sentiment analysis to 281 articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:10<00:00,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment analysis completed successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "load_data_to_bigquery() got an unexpected keyword argument 'df'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# apply anlp for sentiment analysis \u001b[39;00m\n\u001b[32m      7\u001b[39m results_df = analyze_sentiment(transformed_df) \n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[43mload_data_to_bigquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresults_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreplace\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: load_data_to_bigquery() got an unexpected keyword argument 'df'"
     ]
    }
   ],
   "source": [
    "articles = extract_data() \n",
    "\n",
    "# Transform data\n",
    "transformed_df = transform_data(articles) \n",
    "\n",
    "# apply anlp for sentiment analysis \n",
    "results_df = analyze_sentiment(transformed_df) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# continue changing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## code to write results to the big query "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Authenticating from big query \n",
    "from google.cloud import bigquery\n",
    "import os \n",
    "\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"service_account.json\"\n",
    "# Initialize BigQuery client\n",
    "client = bigquery.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "load_dotenv()\n",
    "project_id = os.getenv(\"project_id\")\n",
    "dataset_id = os.getenv(\"dataset_id\")\n",
    "table_id = os.getenv(\"table_id\")\n",
    "table_ref = f\"{project_id}.{dataset_id}.{table_id}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "    SELECT  *\n",
    "    FROM `{table_ref}`\n",
    "    ORDER BY publishedAt DESC;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the query\n",
    "query_job = client.query(query)\n",
    "\n",
    "# Fetch results\n",
    "results_gcp = query_job.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_gcp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results__read_gcp_df =results_gcp.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results__read_gcp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.oauth2 import service_account\n",
    "# credentials = service_account.Credentials.from_service_account_file(\n",
    "#     './service_account.json'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df['publishedAt'] = pd.to_datetime(results_df['publishedAt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df.to_gbq(\n",
    "#     destination_table='news_dataset.news_articles',  # Replace with your dataset and desired table name\n",
    "#     project_id=project_id,  # Replace with your actual project ID\n",
    "#     if_exists='append',  # Change to 'append' if you want to add to existing data\n",
    "#     credentials=credentials\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(results_df['publishedAt'].dtype)\n",
    "# print(results_df['publishedAt'].head())\n",
    "# # Check for null values\n",
    "# print(results_df['publishedAt'].isna().sum())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "news_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
